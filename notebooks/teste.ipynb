{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4ab6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "\n",
    "# --- 1. Inicialização da Sessão Spark ---\n",
    "# Configura a conexão com o cluster Spark, MinIO e habilita o Delta Lake.\n",
    "print(\"--- 1/6: Inicializando a Sessão Spark ---\")\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"AnaliseChurnComMLflow\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"admin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"mysecretpassword\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Sessão Spark iniciada com sucesso!\")\n",
    "\n",
    "# --- 2. Geração de Dados Genéricos ---\n",
    "# Criamos um DataFrame Pandas com dados fictícios de clientes.\n",
    "print(\"\\n--- 2/6: Gerando dados de clientes (churn) ---\")\n",
    "dados_ficticios = {\n",
    "    'cliente_id': range(1, 101),\n",
    "    'idade': [_ for _ in range(20, 70)] * 2,\n",
    "    'tempo_de_uso_meses': [int(i/2) + 1 for i in range(100)],\n",
    "    'valor_mensal': [50 + (i % 20) * 5 for i in range(100)],\n",
    "    # Clientes com mais tempo de uso e valor mais alto têm menos chance de cancelar\n",
    "    'churn': [1 if i < 20 or (i % 10 == 0 and i > 50) else 0 for i in range(100)]\n",
    "}\n",
    "pandas_df = pd.DataFrame(dados_ficticios)\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "print(\"Dados gerados. Amostra:\")\n",
    "spark_df.show(5)\n",
    "\n",
    "# --- 3. Salvando a Tabela no MinIO (Data Lake) ---\n",
    "# O bucket é 'bases' e a tabela se chamará 'dados_clientes'\n",
    "print(\"\\n--- 3/6: Salvando dados como tabela Delta no MinIO ---\")\n",
    "delta_path = \"s3a://bases/dados_clientes\"\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "print(f\"Tabela salva com sucesso em: {delta_path}\")\n",
    "\n",
    "# --- 4. Carregando a Tabela do MinIO ---\n",
    "# Confirmamos que os dados podem ser lidos de volta.\n",
    "print(\"\\n--- 4/6: Lendo dados da tabela Delta ---\")\n",
    "df_lido = spark.read.format(\"delta\").load(delta_path)\n",
    "print(f\"Total de {df_lido.count()} registros lidos do MinIO.\")\n",
    "\n",
    "# --- 5. Análise e Treinamento com MLflow ---\n",
    "print(\"\\n--- 5/6: Treinando modelo de Machine Learning e registrando no MLflow ---\")\n",
    "\n",
    "# Conecta ao servidor MLflow que está rodando no Docker\n",
    "mlflow.set_tracking_uri(\"http://mlflow:5000\")\n",
    "\n",
    "# Define um nome para o nosso \"projeto\" de experimentos\n",
    "experiment_name = \"Previsao_Churn_Clientes\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Converte de volta para Pandas para usar com Scikit-learn\n",
    "df_treino = df_lido.toPandas()\n",
    "\n",
    "# Separa os dados em features (X) e alvo (y)\n",
    "features = ['idade', 'tempo_de_uso_meses', 'valor_mensal']\n",
    "target = 'churn'\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_treino[features], df_treino[target], test_size=0.3, random_state=42)\n",
    "\n",
    "# Inicia uma \"run\" no MLflow para registrar tudo\n",
    "with mlflow.start_run() as run:\n",
    "    print(f\"Iniciando run do MLflow: {run.info.run_id}\")\n",
    "    \n",
    "    # Parâmetros do modelo\n",
    "    C_param = 1.0\n",
    "    solver_param = 'liblinear'\n",
    "    \n",
    "    # Cria e treina o modelo de Regressão Logística\n",
    "    model = LogisticRegression(C=C_param, solver=solver_param)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Faz previsões e calcula a acurácia\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    # Registra tudo no MLflow\n",
    "    mlflow.log_param(\"regularization_strength_C\", C_param)\n",
    "    mlflow.log_param(\"solver\", solver_param)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Salva o modelo treinado como um artefato no MLflow (que por sua vez salva no MinIO)\n",
    "    mlflow.sklearn.log_model(model, \"modelo_regressao_logistica\")\n",
    "    \n",
    "    print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "    print(\"Parâmetros, métricas e modelo registrados no MLflow!\")\n",
    "\n",
    "# --- 6. Finalização ---\n",
    "print(\"\\n--- 6/6: Encerrando a sessão Spark ---\")\n",
    "spark.stop()\n",
    "print(\"Processo concluído!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
