{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b05f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Criar a SparkSession com suporte ao Hive\n",
    "# As configurações já estão no ambiente, então o código fica limpo!\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"HiveMetastore-Test\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .enableHiveSupport()  # Habilita a integração com o Hive Metastore\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session com Hive Metastore criada!\")\n",
    "\n",
    "# 2. Criar um DataFrame de exemplo\n",
    "data = [(\"Fusca\", 1970), (\"Opala\", 1980), (\"Chevette\", 1985)]\n",
    "columns = [\"modelo\", \"ano\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# 3. Salvar o DataFrame como uma tabela gerenciada\n",
    "# O Spark irá gerenciar os metadados (no PostgreSQL) e os dados (no MinIO)\n",
    "table_name = \"carros_classicos\"\n",
    "print(f\"Salvando DataFrame como tabela gerenciada: '{table_name}'\")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "print(\"Tabela salva com sucesso!\")\n",
    "\n",
    "# 4. Ler a tabela de volta usando spark.read.table()\n",
    "print(f\"\\nLendo a tabela '{table_name}' do catálogo:\")\n",
    "df_lido = spark.read.table(table_name)\n",
    "df_lido.show()\n",
    "\n",
    "# 5. Você pode listar as tabelas existentes\n",
    "print(\"\\nTabelas no catálogo:\")\n",
    "spark.catalog.listTables().show()\n",
    "\n",
    "# Você pode fechar este notebook, reiniciar o kernel ou até mesmo os contêineres.\n",
    "# Ao abrir um novo notebook e executar a célula 4 novamente, a tabela ainda estará lá!\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef80a1b-89f8-4a25-9490-acccfe550e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Inicie a SparkSession com suporte ao Hive\n",
    "# Lembre-se que todas as configurações já estão no ambiente, então o código fica limpo.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Gerenciando-Databases\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .enableHiveSupport()  # Essencial para interagir com o catálogo do Hive\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session com Hive Metastore criada!\")\n",
    "\n",
    "# 2. Crie um novo database chamado 'bronze_layer'\n",
    "# Usar 'IF NOT EXISTS' é uma boa prática para evitar erros se o database já existir.\n",
    "db_name = \"bronze_layer\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} COMMENT 'Database para dados brutos'\")\n",
    "\n",
    "print(f\"\\nDatabase '{db_name}' criado com sucesso!\")\n",
    "\n",
    "# 3. Verifique se o database foi criado listando todos os databases\n",
    "print(\"\\nDatabases existentes no catálogo:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# 4. Mude o contexto para o novo database\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "print(f\"\\nContexto alterado para o database '{db_name}'.\")\n",
    "\n",
    "# 5. Crie uma tabela dentro do novo database\n",
    "# Os dados desta tabela serão armazenados em MinIO no caminho:\n",
    "# s3a://database/warehouse/bronze_layer.db/clientes_brutos\n",
    "data = [(\"1\", \"John Doe\", \"2024-10-26\"), (\"2\", \"Jane Smith\", \"2024-10-27\")]\n",
    "columns = [\"id_cliente\", \"nome_completo\", \"data_cadastro\"]\n",
    "df_clientes = spark.createDataFrame(data, columns)\n",
    "\n",
    "df_clientes.write.mode(\"overwrite\").saveAsTable(\"clientes_brutos\")\n",
    "\n",
    "print(\"\\nTabela 'clientes_brutos' criada dentro do database 'bronze_layer'.\")\n",
    "\n",
    "# 6. Liste as tabelas no database atual para confirmar\n",
    "print(f\"\\nTabelas no database '{db_name}':\")\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Para acessar a tabela, você pode usar o nome qualificado: bronze_layer.clientes_brutos\n",
    "# ou, como já estamos usando o database, apenas o nome da tabela.\n",
    "print(\"\\nLendo dados da nova tabela:\")\n",
    "spark.read.table(\"clientes_brutos\").show()\n",
    "\n",
    "# Lembre-se de parar a sessão ao final\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9643aae-c8f3-4f07-91f6-aa596234b07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: javax.jdo.option.ConnectionURL\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/24 20:03:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session com Hive Metastore criada!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Inicie a SparkSession com suporte ao Hive\n",
    "# Lembre-se que todas as configurações já estão no ambiente, então o código fica limpo.\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Gerenciando-Databases2\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .enableHiveSupport()  # Essencial para interagir com o catálogo do Hive\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark Session com Hive Metastore criada!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90103785-26c4-462f-be18-dc15045ec9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Crie um novo database chamado 'bronze_layer'\n",
    "# Usar 'IF NOT EXISTS' é uma boa prática para evitar erros se o database já existir.\n",
    "db_name = \"bronze_layer\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} COMMENT 'Database para dados brutos'\")\n",
    "\n",
    "print(f\"\\nDatabase '{db_name}' criado com sucesso!\")\n",
    "\n",
    "\n",
    "# 4. Mude o contexto para o novo database\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "print(f\"\\nContexto alterado para o database '{db_name}'.\")\n",
    "\n",
    "# 5. Crie uma tabela dentro do novo database\n",
    "# Os dados desta tabela serão armazenados em MinIO no caminho:\n",
    "# s3a://database/warehouse/bronze_layer.db/clientes_brutos\n",
    "input_path = f\"s3a://raw/cad_fi_hist_admin.csv\"\n",
    "df_clientes = spark.read.csv(input_path, sep=\";\", header=True)\n",
    "df_clientes.show()\n",
    "# df_clientes.write.mode(\"overwrite\").saveAsTable(\"clientes_brutos\")\n",
    "\n",
    "# print(\"\\nTabela 'clientes_brutos' criada dentro do database 'bronze_layer'.\")\n",
    "\n",
    "# # 6. Liste as tabelas no database atual para confirmar\n",
    "# print(f\"\\nTabelas no database '{db_name}':\")\n",
    "# spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# # Para acessar a tabela, você pode usar o nome qualificado: bronze_layer.clientes_brutos\n",
    "# # ou, como já estamos usando o database, apenas o nome da tabela.\n",
    "# print(\"\\nLendo dados da nova tabela:\")\n",
    "# spark.read.table(\"clientes_brutos\").show()\n",
    "\n",
    "# # Lembre-se de parar a sessão ao final\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a2ad36-8db1-4483-b3a2-70b7f9f5d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_clientes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636741f8-de34-49b2-bfc6-5dc4ffe3aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Database 'bronze_layer' criado com sucesso!\n",
      "\n",
      "Contexto alterado para o database 'bronze_layer'.\n",
      "+--------+---------+-------------------+------+----------------+------------------+\n",
      "| ORDERID|BRANCH_ID|              DATE_|USERID|     NAMESURNAME|       TOTALBASKET|\n",
      "+--------+---------+-------------------+------+----------------+------------------+\n",
      "| 7905270|  320-DE1|2022-08-22 00:00:00| 72946|       Ali İlhan|2637,5499999999997|\n",
      "| 8131447|   56-AN4|2022-06-05 00:00:00| 58126|      Aysun Dinç|           2262,06|\n",
      "|10176430|  348-MU1|2023-01-02 00:00:00| 41317|     Taner Yavuz|           2195,54|\n",
      "| 8445704|   39-AY3|2021-01-28 00:00:00| 39303| Esra Lara Keleş|            446,86|\n",
      "| 8616360|  777-YA1|2022-10-24 00:00:00| 64870|       Ela Çakır|            430,18|\n",
      "| 7369024|  716-BU3|2021-05-21 00:00:00| 89153|     Selin İlhan|            996,81|\n",
      "| 7656284|   56-AN1|2022-12-25 00:00:00| 55529|   Yelda Erdoğan|            784,26|\n",
      "| 7468769|  146-KA3|2023-04-29 00:00:00| 23596|    Hatice Çoban|            402,68|\n",
      "| 8398865|  734-İS1|2021-12-31 00:00:00| 99068|      Berk Koçak|           1651,96|\n",
      "| 9203327|  734-İS1|2021-12-27 00:00:00| 67055|     Kadir Aydın|            109,32|\n",
      "| 6160525|  225-ER2|2022-06-05 00:00:00|  8257|    Onur Erdoğan|            759,53|\n",
      "| 9375427|  463-ŞA2|2021-06-28 00:00:00| 86319|    Selin Sağlam|3027,9900000000002|\n",
      "| 9413824|  447-MA1|2023-05-30 00:00:00| 42439|Seda Suna Karaca|            1490,5|\n",
      "| 9976553|  652-OR2|2021-05-02 00:00:00| 76015|    Melike Uysal|           2014,64|\n",
      "| 7912453|  657-SI1|2023-06-07 00:00:00| 79172|     Özgür Koçak|1878,9099999999999|\n",
      "| 9370883|  754-SA2|2022-07-20 00:00:00| 64244|    Yelda Öztürk|           1849,63|\n",
      "| 8627111|   56-AN1|2021-05-17 00:00:00| 83654|     Meryem Acar|           1171,92|\n",
      "| 7745920|   33-AF1|2021-12-23 00:00:00| 17277|    Gönül Yılmaz|            251,18|\n",
      "| 8067881|  421-DI1|2021-12-21 00:00:00| 56989|   Rabia Kalaycı|           1164,05|\n",
      "| 9097670|  421-DI2|2021-11-25 00:00:00| 86322|     Berkay Özer|            723,45|\n",
      "+--------+---------+-------------------+------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/24 20:30:07 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "25/10/24 20:30:07 WARN HiveConf: HiveConf of name hive.internal.ss.authz.settings.applied.marker does not exist\n",
      "25/10/24 20:30:07 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "25/10/24 20:30:07 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Crie um novo database chamado 'bronze_layer'\n",
    "# Usar 'IF NOT EXISTS' é uma boa prática para evitar erros se o database já existir.\n",
    "db_name = \"bronze_layer\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} COMMENT 'Database para dados brutos'\")\n",
    "\n",
    "print(f\"\\nDatabase '{db_name}' criado com sucesso!\")\n",
    "\n",
    "\n",
    "# 4. Mude o contexto para o novo database\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "print(f\"\\nContexto alterado para o database '{db_name}'.\")\n",
    "\n",
    "# 5. Crie uma tabela dentro do novo database\n",
    "# Os dados desta tabela serão armazenados em MinIO no caminho:\n",
    "# s3a://database/warehouse/bronze_layer.db/clientes_brutos\n",
    "input_path = f\"s3a://raw/3A_Superstore/Orders.csv\"\n",
    "df_clientes = spark.read.csv(input_path, header=True)\n",
    "df_clientes.show()\n",
    "df_clientes.write.mode(\"overwrite\").saveAsTable(\"Orders\")\n",
    "# df_clientes.write.mode(\"overwrite\").saveAsTable(\"clientes_brutos\")\n",
    "\n",
    "# print(\"\\nTabela 'clientes_brutos' criada dentro do database 'bronze_layer'.\")\n",
    "\n",
    "# # 6. Liste as tabelas no database atual para confirmar\n",
    "# print(f\"\\nTabelas no database '{db_name}':\")\n",
    "# spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# # Para acessar a tabela, você pode usar o nome qualificado: bronze_layer.clientes_brutos\n",
    "# # ou, como já estamos usando o database, apenas o nome da tabela.\n",
    "# print(\"\\nLendo dados da nova tabela:\")\n",
    "# spark.read.table(\"clientes_brutos\").show()\n",
    "\n",
    "# # Lembre-se de parar a sessão ao final\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ce6fb0-ff44-45db-bef0-cb79ea3db90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==============>                                         (6 + 18) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10235193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(df_clientes.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69fe4ad3-9615-4b5f-a0d1-4a6289ccc64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+------+---------+------------------+------+--------+\n",
      "|ORDERID|ORDERDETAILID|AMOUNT|UNITPRICE|        TOTALPRICE|ITEMID|ITEMCODE|\n",
      "+-------+-------------+------+---------+------------------+------+--------+\n",
      "|5523363|     27622768|     4|     12,5|             25,96| 23667|   40199|\n",
      "|5503074|     27521136|     3|      3,5|             10,08|  1324|    2871|\n",
      "|5582877|     27921491|     4|      4,9|             35,04|  4735|   17605|\n",
      "|5571722|     27865830|     8|     15,5|             85,84| 17179|   32797|\n",
      "|5635480|     28184303|     2|     57,5|             121,1| 21009|   34535|\n",
      "|5614687|     28079597|     4|     14,7|              54,4| 22870|   44617|\n",
      "|5700834|     28511213|     8|       15|            123,36| 22075|   20934|\n",
      "|5684943|     28431460|     8|     13,5|               122|  2434|    8106|\n",
      "|5684943|     28431459|     4|      9,4|             41,84|  4737|   17608|\n",
      "|5684943|     28431462|     4|     10,5|             47,52| 13263|   14368|\n",
      "|5685078|     28432163|     8|       18|             35,36| 10180|   10949|\n",
      "|5819558|     29104887|     4|        3|             11,28|  6842|    6821|\n",
      "|5786963|     28941779|     4|     14,7|             40,36| 10765|    4671|\n",
      "|5786963|     28941775|     6|     35,4|            143,16|  5219|    5822|\n",
      "|5872124|     29367755|     1|     8,75|              6,58| 16466|   30198|\n",
      "|5888580|     29449633|     4|     6,05|              25,2|  6870|    6870|\n",
      "|5860279|     29308978|     5|       39|140,85000000000002| 23404|   39709|\n",
      "|5860358|     29309424|     8|     8,85|             74,96|  7706|   23389|\n",
      "|5860358|     29309423|     3|      210|            688,08| 15892|   27299|\n",
      "|5860379|     29309539|     8|      2,9|             13,92|  3183|   15186|\n",
      "+-------+-------------+------+---------+------------------+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51185032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "input_path = f\"s3a://raw/3A_Superstore/Order_Details.csv\"\n",
    "df_Order_Details = spark.read.csv(input_path, header=True)\n",
    "df_Order_Details.show()\n",
    "print(df_Order_Details.count())\n",
    "df_Order_Details.write.mode(\"overwrite\").saveAsTable(\"Order_Details\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08957939-f7ca-41ff-b6da-82610a8a7fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos encontrados em 's3a://raw/3A_Superstore/':\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|caminho_completo                          |\n",
      "+------------------------------------------+\n",
      "|s3a://raw/3A_Superstore/Order_Details.csv |\n",
      "|s3a://raw/3A_Superstore/Orders.csv        |\n",
      "|s3a://raw/3A_Superstore/Categories.csv    |\n",
      "|s3a://raw/3A_Superstore/Customers_ENG.csv |\n",
      "|s3a://raw/3A_Superstore/Customers.csv     |\n",
      "|s3a://raw/3A_Superstore/Categories_ENG.csv|\n",
      "|s3a://raw/3A_Superstore/Branches.csv      |\n",
      "|s3a://raw/3A_Superstore/Branches_ENG.csv  |\n",
      "+------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============>                                          (6 + 18) / 24]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lista de Arquivos (Python):\n",
      "s3a://raw/3A_Superstore/Order_Details.csv\n",
      "s3a://raw/3A_Superstore/Orders.csv\n",
      "s3a://raw/3A_Superstore/Categories.csv\n",
      "s3a://raw/3A_Superstore/Customers_ENG.csv\n",
      "s3a://raw/3A_Superstore/Customers.csv\n",
      "s3a://raw/3A_Superstore/Categories_ENG.csv\n",
      "s3a://raw/3A_Superstore/Branches.csv\n",
      "s3a://raw/3A_Superstore/Branches_ENG.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import input_file_name\n",
    "\n",
    "# Defina o bucket e o prefixo (diretório) que você quer listar\n",
    "bucket_name = \"raw\"\n",
    "prefix_path = \"3A_Superstore/\" # Deixe vazio para a raiz do bucket\n",
    "\n",
    "# Use um curinga (glob) para que o Spark encontre todos os arquivos\n",
    "# O formato \"text\" é leve e bom para isso.\n",
    "try:\n",
    "    path_para_ler = f\"s3a://{bucket_name}/{prefix_path}*\"\n",
    "\n",
    "    # O Spark lê os arquivos (apenas o suficiente para obter os metadados do caminho)\n",
    "    df_files = spark.read.format(\"text\").load(path_para_ler)\n",
    "\n",
    "    # Use input_file_name() para obter o caminho completo e distinct() para evitar duplicatas\n",
    "    df_file_list = df_files.select(input_file_name().alias(\"caminho_completo\")).distinct()\n",
    "\n",
    "    print(f\"Arquivos encontrados em 's3a://{bucket_name}/{prefix_path}':\")\n",
    "    df_file_list.show(truncate=False)\n",
    "\n",
    "    # Se você quiser a lista em formato Python:\n",
    "    file_list = [row.caminho_completo for row in df_file_list.collect()]\n",
    "    print(\"\\nLista de Arquivos (Python):\")\n",
    "    for f in file_list:\n",
    "        print(f)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao listar arquivos: {e}\")\n",
    "    print(\"Verifique se o bucket e o caminho existem e se as credenciais estão corretas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e045f3ba-7ae8-4672-ba44-5c60d8f6d9cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
